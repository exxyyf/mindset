{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f02abb",
   "metadata": {},
   "source": [
    "# Классификация мебели внутри интерьера с помощью нейросети, дообученной на синтетических данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17760fb3",
   "metadata": {},
   "source": [
    "**Описание проекта:**\n",
    "\n",
    "Целью данного проекта является создание рабочей нейросети для классификации видов мебели на снимках интерьера, по заданию для обучения модели требуется подготовить синтетический датасет из 1000 изображений, на котором нужно дообучить предобученную нейросеть.\n",
    "\n",
    "\n",
    "**Состав проекта:**\n",
    "\n",
    "Данный проект можно условно поделить на две задачи: \n",
    "- создание датасета\n",
    "- создание нейросети\n",
    "<br></br>\n",
    "\n",
    "Таким образом, состав проекта следующий:\n",
    "1. **Создание датасета**\n",
    "  - создание 3D модели интерьера в Unity\n",
    "  - настройка рандомизации положения камеры с помощью модуля Unity Perception и кастомных скриптов\n",
    "  - настройка аннотации изображений (всего 3 класса мебели - стул, стол, диван)\n",
    "  - настройка сценария и создание датасета\n",
    "  - перевод датасета в формат Common Objects in Context (COCO)\n",
    " \n",
    "2. **Создание нейросети**\n",
    " - создание нейросети на основе CNN в Pytorch с выходными нейронами согласно числу классов (всего классов получилось 7, так как решаем задачу классификации объектов: 3 класса одиночных предметов, еще 3 класса их комбинаций типа \"стол + стул\", один класс для случаев, когда все три типа находятся в кадре)\n",
    " - тестирование нейросети на реальных изображениях интерьеров"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b2289f",
   "metadata": {},
   "source": [
    "<h2>Создание датасета</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd2c48f",
   "metadata": {},
   "source": [
    "<h3>Создание модели интерьера в Unity</h3>\n",
    "\n",
    "С помощью программы 3dsMax я создала комнату с окнами и назначила ей материалы. Затем, с помощью экспорта в формат .fbx она была перенесена в Unity. Далее путем загрузки моделей мебели в 3DsMax, экспорта их в .fbx и импорта их в Unity были размещены предметы мебели. \n",
    "\n",
    "При выполнении данной задачи я столкнулась с трудностями поддержки некоторых материалов и текстур - их пришлось заново назначать в Unity, формат .fbx, к сожалению, не позволил произвести импорт чисто."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7842a1b6",
   "metadata": {},
   "source": [
    "[](https://pasteboard.co/lnr1KPHx1yI4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1735171f",
   "metadata": {},
   "source": [
    "<h3>Настройка рандомизации положения камеры</h3>\n",
    "\n",
    "Далее с помощью модуля **Unity Perception** я назначила камеру, установила рандомизацию камеры, создала сценарий ее перемещения и формат выходных изображений 224px x 224px. Модуль **Perception** позволяет также настроить рандомизацию цветов предметов, в данном случае применяю рандомизацию цвета стен и мебели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d65ea1",
   "metadata": {},
   "source": [
    "*Скрипт настройки поворота камеры:*\n",
    "```c#\n",
    "using System;\n",
    "using System.Collections;\n",
    "using System.Collections.Generic;\n",
    "using UnityEngine;\n",
    "using UnityEngine.Perception.Randomization.Parameters;\n",
    "using UnityEngine.Perception.Randomization.Randomizers;\n",
    "\n",
    "[Serializable]\n",
    "[AddRandomizerMenu(\"Perception/Camera Randomizer2\")]\n",
    "\n",
    "public class CameraRandomizer2 : Randomizer\n",
    "{\n",
    "    public FloatParameter cameraYRotation;\n",
    "    public Camera mainCamera;\n",
    "\n",
    "    protected override void OnIterationStart()\n",
    "    {\n",
    "        var elevation = cameraYRotation.Sample();\n",
    "\n",
    "        mainCamera.transform.rotation = Quaternion.Euler(0f, elevation, 0f);\n",
    "        \n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc0b593",
   "metadata": {},
   "source": [
    "<h3>Настройка аннотации изображений</h3>\n",
    "\n",
    "В Perception имеются несколько режимов аннотации изображений - создание BoundingBox, масок для сегментации изображений. Для конкретной задачи классификации был выбран **BoundingBox2DLabeler**, в котором я указала три класса - `table`, `sofa`, `chair`.\n",
    "\n",
    "Итогом работы данного лейблера стал json-файл, в котором были указаны классы предметов, находящихся в кадре, и координаты описывающих их прямоугольников."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0098a17f",
   "metadata": {},
   "source": [
    "Ниже приведена схема устройства аннотации файлов в Unity Perception:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7a8c26e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"[image.png]\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06926ca1",
   "metadata": {},
   "source": [
    "Результатом работы с Unity стал датасет с трехканальными цветными rgb-изображениями разных ракурсов интерьера с мебелью внутри в количестве 1000 штук и файлы аннотаций в формате json.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b773589",
   "metadata": {},
   "outputs": [],
   "source": [
    "![image-3.png](attachment:image-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ce732b",
   "metadata": {},
   "source": [
    "<h3>Конвертация датасета в формат Common Objects in Context (COCO)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d456e35",
   "metadata": {},
   "source": [
    "Для выполнения данной процедуры были опробованы несколько инструментов. Успешным оказался [код](https://github.com/lessw2020/perception_tools/blob/main/perception_to_coco.ipynb) с просторов гитхаб:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1557fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5673d363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/User/Desktop/mindset')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path, PurePath\n",
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32bd3ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from copy import deepcopy\n",
    "import os\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea9f8d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def require_dir(item, type=\"Dataset\"):\n",
    "    itemp = Path(item)\n",
    "    if not itemp.is_dir():\n",
    "        raise ValueError(f\"directory of type {type} not found. aborting...\")\n",
    "\n",
    "def get_sub_folders(upper_path):\n",
    "    # verify upper path\n",
    "    fpath = Path(upper_path)\n",
    "    \n",
    "    require_dir(fpath, \"Perception root dir\")  # will break with raise error if not dir\n",
    "    \n",
    "    # get required subfolders\n",
    "    # dataset\n",
    "    #subdirs = [f.path for f in os.scandir(upper_path) if f.is_dir() ]\n",
    "    \n",
    "    dataset_glob = fpath.glob(\"Dataset*\")\n",
    "    try:\n",
    "        dataset_dir = next(dataset_glob) \n",
    "    except StopIteration:\n",
    "        print(f\"** failed to get Dataset dir. aborting..\")\n",
    "        return None, None\n",
    "    \n",
    "    require_dir(dataset_dir, \"Dataset\")\n",
    "    \n",
    "    print(f\"--> using {dataset_dir.name} to generate annotations\")\n",
    "    \n",
    "    image_glob = fpath.glob(\"RGB*\")\n",
    "    \n",
    "    try:\n",
    "        image_dir = next(image_glob)\n",
    "    except StopIteration:\n",
    "        print(f\"** failed to get an image dir. aborting...\")\n",
    "        return None, None\n",
    "        \n",
    "    \n",
    "    print(f\"--> using {image_dir.name} for images\")\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "        \n",
    "    return image_dir, dataset_dir\n",
    "def get_anno_file(dataset_dir):\n",
    "    anno_file = dataset_dir/'annotation_definitions.json'\n",
    "    return anno_file\n",
    "def open_json(fpath):\n",
    "    try:\n",
    "        with open(str(fpath)) as f:\n",
    "            jh = json.load(f)\n",
    "    except:\n",
    "        raise ValueError(f\"failed to open {fpath} for read\")\n",
    "    return jh\n",
    "def get_perception_categories(anno_file, show_info=True, supercategory = \"rdt\"):\n",
    "    \n",
    "    fh = open_json(anno_file)\n",
    "    anno_list = fh['annotation_definitions']\n",
    "    spec = anno_list[0]\n",
    "    spec = spec['spec']\n",
    "    print(f\"\\n--> labels in perception definitions:\\n\")\n",
    "    for item in spec:\n",
    "        print(item)\n",
    "    \n",
    "    print(f\"\\n-->building coco categories:\")\n",
    "    coco_category_block = []\n",
    "    for item in spec:\n",
    "        holding = {}\n",
    "        holding['id']= item['label_id']\n",
    "        \n",
    "        holding['name'] = item['label_name']\n",
    "        holding['supercategory']=supercategory\n",
    "        print(holding)\n",
    "        coco_category_block.append(holding)\n",
    "    return coco_category_block\n",
    "    \n",
    "    \n",
    "def get_perception_annotations(anno_dir, image_width=1024, image_height=768):\n",
    "    \n",
    "    anno_id = 0  # can also start with 1 if desired\n",
    "    image_id = 0\n",
    "    \n",
    "    images_block = []\n",
    "    annos_block = []\n",
    "    \n",
    "    capture_files = anno_dir.glob('captures*.json')\n",
    "    \n",
    "    for j,_ in enumerate(capture_files):\n",
    "        pass\n",
    "    \n",
    "    print(f\"\\n--> {j+1} capture files detected. Processing...\")\n",
    "    \n",
    "    # processing\n",
    "    capture_files = anno_dir.glob('captures*.json')\n",
    "    \n",
    "    for item in capture_files:\n",
    "    \n",
    "        fh = open_json(item)\n",
    "        \n",
    "        captures = fh['captures']\n",
    "        \n",
    "        for image_entry in captures:\n",
    "            image_dict = {}\n",
    "            \n",
    "            image_dict['id'] = image_id\n",
    "            \n",
    "            fp = Path(image_entry['filename'])\n",
    "            \n",
    "            # it's likely you are exporting all images as same size.\n",
    "            # could open each image and check height/width, but will use passed in args for now\n",
    "            image_dict['width'] = image_width\n",
    "            image_dict['height'] = image_height\n",
    "            image_dict['filename'] = fp.name\n",
    "            \n",
    "            # dummy values \n",
    "            image_dict['license'] = None\n",
    "            image_dict['flickr_url'] = \"\"\n",
    "            image_dict['coco_url'] = \"\"\n",
    "            image_dict['date_captured'] = \"0:00\"\n",
    "            \n",
    "            images_block.append(deepcopy(image_dict))\n",
    "            \n",
    "            annos = image_entry['annotations']\n",
    "            \n",
    "            ad = {}\n",
    "            \n",
    "            for a in annos:\n",
    "                ad.update(a)\n",
    "            \n",
    "            values = ad['values']\n",
    "                \n",
    "            coco_anno = []\n",
    "            \n",
    "            for item in values:\n",
    "                temp_anno={}\n",
    "                x = item['x']\n",
    "                y = item['y']\n",
    "                w = item['width']\n",
    "                h = item['height']\n",
    "                \n",
    "                bbox = [x,y,w,h]\n",
    "                \n",
    "                temp_anno[\"id\"] = anno_id\n",
    "                temp_anno[\"image_id\"] = image_id\n",
    "                temp_anno[\"category_id\"] = item[\"label_id\"]\n",
    "                \n",
    "                #bbox details\n",
    "                temp_anno[\"bbox\"] = bbox\n",
    "                temp_anno[\"area\"] = int(w*h)\n",
    "                \n",
    "                #segmentation - todo if needed\n",
    "                temp_anno[\"segmentation\"] = None\n",
    "                temp_anno[\"iscrowd\"] = 0\n",
    "                \n",
    "                annos_block.append(deepcopy(temp_anno))\n",
    "\n",
    "                anno_id +=1\n",
    "            \n",
    "            image_id+=1\n",
    "        \n",
    "    print(f\"--> annotation processing completed.\")\n",
    "    return images_block, annos_block\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ed696b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_perception(base_dir, out_file=\"coco_labels.json\", image_width=224, image_height=224):\n",
    "    \"\"\" \n",
    "    main entry for converting perception output into ready to train coco file\n",
    "    \n",
    "    note - currently image height and width are passed in /hardcoded.  Workaround is can open every image file\n",
    "    and check, or update perception to export image info...for now just using passed in vars\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    image_dir, dataset_dir = get_sub_folders(base_dir)\n",
    "    \n",
    "    mainfile = {}\n",
    "    \n",
    "    #build info section\n",
    "    infod = {}\n",
    "    \n",
    "    today = dt.today()\n",
    "    infod['year'] = str(today.year)\n",
    "    infod['date_created'] = str(today)\n",
    "\n",
    "    infod['version']= \"1.0\"\n",
    "    infod['contributor']=\"lessw2020\"\n",
    "    infod['url']='https://github.com/lessw2020/perception_tools'\n",
    "    \n",
    "    mainfile['info'] = infod\n",
    "    \n",
    "    mainfile['licenses'] = []\n",
    "    \n",
    "    # get categories\n",
    "    perception_anno = get_anno_file(dataset_dir)\n",
    "    \n",
    "    coco_cats = get_perception_categories(perception_anno)\n",
    "    \n",
    "    mainfile['categories'] = coco_cats\n",
    "    \n",
    "    #print(f\"--> mainfile = {mainfile}\")\n",
    "    \n",
    "    # get annotations\n",
    "    images_block, annos_block = get_perception_annotations(dataset_dir, \n",
    "                                                           image_width, image_height)\n",
    "    \n",
    "    mainfile['images']=images_block\n",
    "    mainfile['annotations'] = annos_block\n",
    "    \n",
    "    if out_file:\n",
    "        save_file = dataset_dir/out_file\n",
    "        with open(save_file,'w') as fh:\n",
    "            json.dump(mainfile, fh)\n",
    "    \n",
    "    # all done\n",
    "    print(f\"\\n--> Processing complete.  Total images = {len(mainfile['images'])}\\n\")\n",
    "    \n",
    "    return mainfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92f8b6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "perception_folder = r\"C:\\Users\\User\\AppData\\LocalLow\\DefaultCompany\\NikosRocks\\f56f0963-1311-4f80-ae43-3dd8a7be9f41\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7cd931ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> using Datasete43ce145-475c-4a44-b1d8-cdb20523c456 to generate annotations\n",
      "--> using RGB4f8edf59-15fe-4565-aa24-0ec21523f501 for images\n",
      "\n",
      "--> labels in perception definitions:\n",
      "\n",
      "{'label_id': 1, 'label_name': 'sofa'}\n",
      "{'label_id': 2, 'label_name': 'table'}\n",
      "{'label_id': 3, 'label_name': 'chair'}\n",
      "\n",
      "-->building coco categories:\n",
      "{'id': 1, 'name': 'sofa', 'supercategory': 'rdt'}\n",
      "{'id': 2, 'name': 'table', 'supercategory': 'rdt'}\n",
      "{'id': 3, 'name': 'chair', 'supercategory': 'rdt'}\n",
      "\n",
      "--> 7 capture files detected. Processing...\n",
      "--> annotation processing completed.\n",
      "\n",
      "--> Processing complete.  Total images = 1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = convert_perception(perception_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b145f8e",
   "metadata": {},
   "source": [
    "Благодаря этой операции появился файл `coco_labels.json`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a25cbd6f",
   "metadata": {},
   "source": [
    "Как пример, посмотрим на картинку номер 207. На ней изображены стол и стулья:\n",
    "\n",
    "\n",
    "\n",
    "Теперь найдем в `coco_labels.json` информацию об этой картинке по номеру:\n",
    "```json\n",
    "{\"id\": 514, \"image_id\": 207, \"category_id\": 3, \"bbox\": [148.0, 104.0, 40.0, 41.0], \"area\": 1640, \"segmentation\": null, \"iscrowd\": 0}, {\"id\": 515, \"image_id\": 207, \"category_id\": 1, \"bbox\": [218.0, 117.0, 6.0, 40.0], \"area\": 240, \"segmentation\": null, \"iscrowd\": 0}, {\"id\": 516, \"image_id\": 207, \"category_id\": 3, \"bbox\": [0.0, 121.0, 26.0, 47.0], \"area\": 1222, \"segmentation\": null, \"iscrowd\": 0}\n",
    "```\n",
    "\n",
    "Видим категории 3, 3 и 1. Все совпало с нашими аннотациями."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d678d616",
   "metadata": {},
   "source": [
    "<h2>Создание нейросети для решения задачи классификации</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6b8addec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pycocotools.coco import COCO\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ca565199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "55bffcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "cap = dset.CocoCaptions(root = r'C:\\Users\\User\\AppData\\LocalLow\\DefaultCompany\\NikosRocks\\b439e3e4-161e-4828-8892-6c66714279e3\\RGBd2e3832b-9df7-41f9-b9b2-3036457ec407',\n",
    "                                    annFile = r'C:\\Users\\User\\AppData\\LocalLow\\DefaultCompany\\NikosRocks\\b439e3e4-161e-4828-8892-6c66714279e3\\Dataset4025e6d2-cb09-4f03-8144-8ae9f22584aa\\coco_labels.json',\n",
    "                                    transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "31808087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  1000\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples: ', len(cap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fb59b322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CocoCaptions\n",
       "    Number of datapoints: 1000\n",
       "    Root location: C:\\Users\\User\\AppData\\LocalLow\\DefaultCompany\\NikosRocks\\b439e3e4-161e-4828-8892-6c66714279e3\\RGBd2e3832b-9df7-41f9-b9b2-3036457ec407\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e5b824b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'file_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m img, target \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\torchvision\\datasets\\coco.py:48\u001b[0m, in \u001b[0;36mCocoDetection.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, Any]:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mids[index]\n\u001b[1;32m---> 48\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_target(\u001b[38;5;28mid\u001b[39m)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new\\lib\\site-packages\\torchvision\\datasets\\coco.py:40\u001b[0m, in \u001b[0;36mCocoDetection._load_image\u001b[1;34m(self, id)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mid\u001b[39m: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[1;32m---> 40\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadImgs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, path))\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'file_name'"
     ]
    }
   ],
   "source": [
    "img, target = cap[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3255060c",
   "metadata": {},
   "source": [
    "Так как, к сожалению, мною пока не было найдено решение вышеописанной проблемы, все, что было сделано в срок - это датасет с аннотациями. Ниже опишу архитектуру нейронной сети, которую я бы дообучала далее."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc57e9",
   "metadata": {},
   "source": [
    "Была поставлена задача классификации, поэтому, полагаю, что использовать нужно сверточную сеть (как пример, ResNet50 или ResNet34, предобученную на ImageNET, в качестве backbone), в которой в выходном слое находится столько же нейронов, сколько и классов. То есть, в нашем случае, 7. Функцией активации для данного слоя должна быть SoftMax. Также, потребуется задать порог - threshold - вероятности, выше которой будет считаться, что объект находится на изображении. Это нужно, потому что наша нейросеть по сути будет выдавать вероятность нахождения каждого класса на картинке."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "160px",
    "width": "166.989px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Создание синтетических данных и обучение на них модели для классификации мебели в интерьере",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "292.77px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
